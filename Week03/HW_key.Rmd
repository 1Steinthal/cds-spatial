---
title: "Ancient cities and inscriptions"
author: "Adela Sobotkova"
date: "13/01/2021 updated `r format(Sys.time(), '%B %d, %Y')`" 
output:
  rmdformats::readthedown:
  highlight: kate
---

```{r setup, include=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
               cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

In this exercise you will map the ancient equivalent of Twitter data: the ancient inscriptions. Ancient people of class, education, and means liked to advertise their achievements and life milestones as well as their sorrows via the means of texts inscribed in stone. These epigraphic monuments were often placed near inhabited areas, roads, and gathering places where they were likely to attract the largest audience. The location of these self-expressions in space and time is a reasonable indicator of changing economic prosperity of the commissioning communities. In this exercise, you will explore how these ancient inscriptions spatially correspond to the distribution of ancient cities and settlements.  

```{r libraries, include=FALSE}
library(sf)
library(raster)
library(tidyverse)
library(leaflet)
```

# Task 1: Load ancient cities and convert to sf object
John Hanson has created a dataset of all cities in the ancient Mediterranean and made it available online. You will download this dataset and convert it into an sf object in order to compare with the inscriptions on the basis of location.  

* Use `read_csv()` to load `Hanson2016_Cities_OxREP.csv` dataset from the provided URL and assign it to `cities` object

```{r load-cities, eval=FALSE}
cities <- as.data.frame(read_csv("http://oxrep.classics.ox.ac.uk/oxrep/docs/Hanson2016/________________"))
```


```{r load-cities-sol, echo=FALSE}
cities<- as.data.frame(read_csv("http://oxrep.classics.ox.ac.uk/oxrep/docs/Hanson2016/Hanson2016_Cities_OxREP.csv"))
```


... then reproject this data to EPSG 3035
```{r prj-cities, eval=FALSE}
# Convert the table into an sf object on the basis of X and Y columns
cities_sf <- ________(cities, coords = c("Longitude (X)", "Latitude (Y)"))

# Define the projection of Lat/Long coordinates as EPSG 4326
cities_sf4326<- st_set_crs(cities_sf, ________)

# Transform the projection to a 2D projection using EPSG 3035
cities_sf3035<- st_transform(cities_sf4326, ___________)

# Verify the projection is 'projected' not 'geographic'
______(cities_sf3035)
```


```{r prj-cities-sol, echo=FALSE}
# Convert the table into an sf object on the basis of X and Y columns
cities_sf <- st_as_sf(cities, coords = c("Longitude (X)", "Latitude (Y)"))

# Define the projection of Lat/Long coordinates as EPSG 4326
cities_sf4326<- st_set_crs(cities_sf, 4326)

# Transform the projection to a 2D projection using EPSG 3035
cities_sf3035<- st_transform(cities_sf4326, 3035, "+units=m")

# Verify the projection is 'projected' not 'geographic'
st_crs(cities_sf3035)
```
### Question: 
*1. What are the measurement units of the `cities_sf3035` object?*


# Task 2: Create a buffer around each city and inspect the result

As each city and inscription corresponds to a dot on the map, the best way to grab and review the inscriptions will be by creating a buffer around each city and then selecting the inscriptions that fall into the buffer as belonging to the city. 

* Create a buffer around the `cities_sf3035` geometry with `st_buffer()` , setting the `dist` argument to the desired radius of 5000m.
* Plot the first ten buffers and the first ten cities on top for a quick review. Can you tell that the buffers are really 5000m?

```{r buff, eval=FALSE}
# Make buffer of 5 km. Check the units of your object to correctly assign value to dist
cities_5km<- st_buffer(_____,dist = _________)

# Plot the first 10 buffers and cities to check result 
plot(___________(___________)[1:10], col = "yellow")
plot(___________(___________)[1:10], pch=20, cex = 0.1, add = TRUE)

```


```{r buff-sol, echo=FALSE}
#Set buffer zone to 5 km
cities_5km<- st_buffer(cities_sf3035, dist = 5000)

#plot(cities_sf3035$geometry[1:10])
plot(st_geometry(cities_5km)[1:10], col = "yellow")
plot(st_geometry(cities_sf3035)[1:10], pch=20, cex = 0.1, add = TRUE)

```

# Task 3: Verify the city buffers are indeed 5km in radius
Well, a quick review may look ok, but you cannot be sure your buffers work well until you add them to a map with a scale. Verify that your buffers are as big as should be by plotting a sample with tmap and adding a scale.

* Grab the first 10 cities and buffers with the `slice()` function (if you have not already done so above)
* Load `tmap` package and plot the 10 cities and buffers with a scale of 0,5,10,20,40 km. Add names and background for clarity. Do your buffers span 10km across or do they span the universe? (If the latter, recheck your CRS, units, and dist argument)

```{r tmap, eval=FALSE}
# Grab the first 10 elements in the sf object and the buffer
ten_buffers <- ________ %>% slice(1:10)
ten_cities <- ________ %>% slice(1:10)

# Create a quick tmap
library(tmap)
current.mode <- tmap_mode("plot")

tm_shape(ten_buffers)  +
  tm_polygons(col = ________) +
  tm_shape(ten_cities) +
  tm_text("Ancient Toponym", size = ___, auto.placement = 5) +
  tm_dots(col = __________, 
             size = 0.1) +
 tm_scale_bar(breaks = ______________,
               text.size = 16,
               position = c("LEFT", "bottom")) +
  tm_compass(position = c("_________", "top"),
             type = "rose", 
             size = 2) +
  tm_credits(position = c(__________),
             text = ________________) +
  tm_layout(main.title = "Map with a scale",
            bg.color = "beige",
            inner.margins = c(0, 0, 0, 0))

```


```{r tmap-sol, echo=FALSE}
# Grab the first 10 elements in the sf object and the buffer
ten_buffers <- cities_5km %>% slice(1:10)
ten_cities <- cities_sf3035 %>% slice(1:10)

# Create a quick map with tmap package (using projected data with buffers) 
library(tmap)
current.mode <- tmap_mode("plot")
ten_buffers$`Ancient Toponym`
tm_shape(ten_buffers)  +
  tm_polygons(col = "pink") +
  tm_shape(ten_cities) +
  tm_text("Ancient Toponym", size = 1, auto.placement = 5) +
  tm_dots(col = "black", 
             size = 0.1) +
  tm_scale_bar(breaks = c(0,5,10,20, 40),
               text.size = 16,
               position = c("LEFT", "bottom")) +
  tm_compass(position = c("RIGHT", "top"),
             type = "rose", 
             size = 2) +
  tm_credits(position = c("LEFT", "bottom"),
             text = "A. Sobotkova, 2022") +
  tm_layout(main.title = "Sample of buffers with a scale",
            bg.color = "beige",
            inner.margins = c(0, 0, 0, 0))
```
If all went well, you should see a map, where the diameter of each city buffer corresponds to the 10km notch on the scale. But, do you know where in the Mediterranean you are?

## An Alternative View
The `tmap` package allows you also an interactive view if you switch the `tmap_mode()` argument to `"view"`. Try it out and assess its (dis)advantages. Which arguments are retained and which are dropped from your `tm_shape` sequence? 

```{r tmap-view, eval = FALSE}
current.mode <- tmap_mode(___________)
tm_shape(_________) +
  ........

```


```{r tmap-view-sol, echo=FALSE}
current.mode <- tmap_mode("view")
ten_buffers$`Ancient Toponym`
tm_shape(ten_buffers)  +
  tm_polygons(col = "pink") +
  tm_shape(ten_cities) +
  tm_text("Ancient Toponym", size = 1, auto.placement = 5) +
  tm_dots(col = "black", 
             size = 0.1)+
   tm_scale_bar(breaks = c(0,5,10,20, 40),
               text.size = 16,
               position = c("LEFT", "bottom")) +
  tm_compass(position = c("RIGHT", "top"),
             type = "rose", 
             size = 2) +
  tm_credits(position = c("LEFT", "bottom"),
             text = "A. Sobotkova, 2022") +
  tm_layout(main.title = "Sample of buffers with a scale",
            bg.color = "beige",
            inner.margins = c(0, 0, 0, 0))

```

Now you should be able to see that the first ten towns and buffers are from mainland Greece. This flexibility of tmaps is very useful.
            
# Task 4: Download ancient inscriptions and wrangle coordinates into shape 
Let's now look at some data that spatially co-occurs with these ancient places. Below is a link to an online dataset from the Epigraphic Database of Heidelberg of ancient inscriptions from one part of the Mediterranean. These inscriptions combine private and official statements dedicated for personal reasons (commemorating a patron or a family member) or public (dedication of a major building, placement of milestone, etc.). 

The json dataset is hefty with some 12 thousand inscriptions and 74 variables. Coordinates are nested in a single column and may need wrangling. Do tasks deliberately in small steps after you test on subsets lest you overwhelm your R.

* Download the linked file with `download.file()` into a directory where you can find it. 
* The inscriptions dataset is in `.json` format  (not to be confused with geojson!), which is becoming the dominant format for sharing data online, especially if some of the data is nested as is the case here. Use the `jsonlite::fromJSON` function in the library to load it back into R
* Next, use `as_tibble()` to convert into rectangular format.  
* Inspect the dataset
* Check the column names for something that looks like spatial data, either Lat/Long, X/Y or a `coordinates` column.

  - Inspect the column(s) - are the coordinates meaningful? What CRS do they look like? 
  - If their meaning is clear, are they in the format you need them to be for an easy conversion to an sf object?
* Separate the two values into `longitude` and `latitude` columns and convert values to numbers. Although this looks like a straightforward task of separating the two values into separate columns, you will see that there are non-numeric characters present that need to be cleaned up en route. (The column actually is formatted as c(123.4567,123.4577)) Make sure to keep the decimal point. Hint: check the `gsub()`, `grep()` and `str_extract()` functions to apply regular expressions.


```{r inscriptions, eval=FALSE}
# Libraries
library(tidyverse)
library(jsonlite)
library(tidytext)

# Download the file and save as inscriptions.json (consider commenting out after you first run to avoid repeat downloading)
download.file("https://sciencedata.dk/public/b6b6afdb969d378b70929e86e58ad975/EDH_subset_2021-02-15.json", "________/inscriptions.json")

# Load it into R from wherever you put it, and convert into a tibble
list_json <- jsonlite::fromJSON("_______/inscriptions.json")
inscriptions = as_tibble(list_json)

# Check the first couple lines and column names
____(inscriptions)
____(inscriptions)
head(unlist(inscriptions$coordinates))
inscriptions$coordinates[[1]][2]

# Wrangle the coordinates into a 2-column format - practice on a small dataset
i_sm <- inscriptions %>% 
  slice(1:100) %>% 
  separate(col = coordinates, into = c("longitude","latitude"), sep = ",") %>%
  mutate(latitude = as.numeric(gsub("_________","",latitude)),
         longitude = as.numeric(gsub("_________","",longitude))) 

# Apply to the whole dataset, once happy with the result
i <- inscriptions %>% 
  separate(col = coordinates, into = c("longitude","latitude"), sep = ",") %>%
  mutate(latitude = as.numeric(gsub("_________","",latitude)),
         longitude = as.numeric(gsub("_________","",longitude)))


# Check the result of the subset, does the location look reasonable?
leaflet() %>% addTiles() %>% addMarkers(lng=i$longitude,lat=i$latitude)
```

```{r insc-sol, echo=FALSE}
# Libraries
library(tidyverse)
library(jsonlite)
library(tidytext)
library(leaflet)

# Download the file and save as inscriptions.json
download.file("https://sciencedata.dk/public/b6b6afdb969d378b70929e86e58ad975/EDH_subset_2021-02-15.json", "../data/inscriptions.json")

# Load it into R and convert into a tibble
list_json <- jsonlite::fromJSON("../data/inscriptions.json")
inscriptions = as_tibble(list_json)

# Check the first couple lines and column names
head(inscriptions)
names(inscriptions)

# Wrangle the coordinates into useful (decimal) format 
i_sm <- inscriptions %>% 
  slice(1:100) %>% 
  separate(col = coordinates, into = c("longitude","latitude"), sep = ",") %>%
  mutate(latitude = as.numeric(gsub("\\)|\\(|c","",latitude)),
         longitude = as.numeric(gsub("\\)|\\(|c","",longitude)))


# Test where the inscriptions are
leaflet() %>% addTiles() %>% addCircleMarkers(lng=i_sm$longitude,lat=i_sm$latitude)
```

Oooof. That was some serious wrangling!

# Task 5: Convert inscriptions into an sf object
Now that the hard work is done, let's apply the wrangling to the full dataset and clean up the missing coordinates and outlier values.

* Not all coordinates are complete. Remove the rows with missing latitude or longitude
* Some incorrect points have sneaked in! Eliminate data with longitude smaller than 5 and larger than 20 degrees.
* Make the resulting `inscriptions` tibble into an sf object using the newly created and cleaned longitude and latitude column in the `coords` argument. The CRS of the data is 4326.
* Plot your data using st_geometry()

```{r insc-sf, eval=FALSE}
i <- inscriptions %>% 
  separate( __________) %>% 
  mutate(___________) %>% 
  filter(____________)) %>% 
  filter(__________) %>% 
  filter(longitude > 5 && longitude < 20)

# Check longitude range
hist(i$___________)

# Create a sf object
insc_sf4326 <- st_as_sf(i, coords = ___________, crs = _________)
```

```{r insc-sf-sol, echo=FALSE}
# Apply the corrections to the entire dataset
i <- inscriptions %>% 
  separate(col = coordinates, into = c("longitude","latitude"), sep = ",") %>%
  mutate(latitude = as.numeric(gsub("\\)|\\(|c","",latitude)),
         longitude = as.numeric(gsub("\\)|\\(|c","",longitude))) %>% 
  filter(!is.na(longitude)) %>% 
  filter(longitude > 5) %>% 
  filter(longitude < 20)

# Check longitude range
hist(i$longitude)

# Make inscriptions into an sf object defining the coordinate systems as Web Mercator with EPSG 4326
insc_sf4326 <- st_as_sf(i, coords = c("longitude","latitude"), crs = 4326)

# Plot
plot(st_geometry(insc_sf4326))
```


# Task 6: Select inscriptions that fall into the cities' buffer
Now that you have both the cities and inscriptions in the same CRS, you can pick the inscriptions which fall within 5km radius of the ancient places in order to locate "urban" inscriptions. Use the inverse `st_difference` to locate "rural" inscriptions.

To reduce the computational intensity of the final intersection, it is a good idea to limit the dissolved city buffer object only to the area within the convex hull of the inscriptions. For the convex hull, you will need to combine the inscriptions into a MULTIPOINT feature using `st_union()`.

* Ensure that the spatial reference system in `cities_5km` buffer object and `insc_sf4326` is consistent.
* Create a convex hull for the inscriptions and use it to clip the city buffers object. Check the metadata of the `cities_it` object
* Combine the city buffers into a single multipolygon
* Use `st_intersection()` to select only the inscriptions that fall within the buffer object and assign these `insc_urban` object
* Use `st_difference()`  to select inscriptions outside these buffers and create `insc_rural` object
* Plot and inspect the results: are the rural and urban inscriptions where you would expect them?

```{r intersection, eval=FALSE}
# Project the sf object into EPSG3035 so it is consistent with cities and their buffers
insc_sf3035 <- st_transform(________________)


# Create a convex hull around the inscriptions's points dissolved into a MULTIPOINT
insc_ch <- st_convex_hull(_________(insc_sf3035))

# Select cities that fall within the convex hull of the inscriptions
cities_it <- st_intersection(insc_ch, _________(cities_5km))

# Dissolve the 399 city buffers into a single MULTIPOLYGON buffer feature
c_buff <- st_union(cities_it)

# Plot these interim results
plot(___________)
plot(___________, border='red', lwd=2, add = TRUE)
plot(___________, add = TRUE)

# Calculate the number of inscriptions in urban and rural areas. This may take a couple seconds
insc_urban <- ____________(insc_sf3035, c_buff)
insc_rural <- ____________(insc_sf3035, c_buff)
```


```{r intersection-sol, echo=FALSE}
# Project the sf object into EPSG3035 so it is consistent with cities and their buffers
insc_sf3035 <- st_transform(insc_sf4326, crs = 3035) 

# Create a convex hull around the inscriptions' points dissolved into a MULTIPOINT
insc_ch <- st_convex_hull(st_union(insc_sf3035))

# Select cities that fall within the convex hull of the inscriptions
cities_it <- st_intersection(insc_ch, cities_5km$geometry)


# Dissolve the 399 city buffers into a single MULTIPOLYGON buffer feature
c_buff <- st_union(cities_it)

# Plot these interim results
plot(insc_sf3035$geometry)
plot(insc_ch, border='red', lwd=2, add = TRUE)
plot(cities_it, add = TRUE)

# Calculate the number of inscriptions in urban and rural areas. This may take a couple seconds
# Start the clock!
ptm <- proc.time()
insc_urban <- st_intersection(insc_sf3035, c_buff)
# Stop the clock
proc.time() - ptm # ca 39 seconds

ptm <- proc.time()
insc_rural <- st_difference(insc_sf3035, c_buff)
proc.time() - ptm

# Plot the result
plot(insc_rural$geometry, col = 'green')
plot(insc_urban$geometry, col = 'red', add = TRUE)
```


### Question 2: 
*What is the ratio of urban to rural inscriptions?*

 
# Task 7: CHALLENGE - Duplicates and distance to nearest city

Selecting all urban inscriptions by a united buffer object should work swimmingly if you reduce computational intensity. The result of urban and rural inscriptions should add up to the total inscriptions for Italy. 
However, what if you wanted to compare one city against another in a central Italian region where cities are near one another and their buffers overlap, e.g. Rome versus Ostia? Some of the inscriptions may in such case be counted twice. The best way to eliminate duplicates is to select inscriptions on the basis of Voronyi polygons instead of buffers. But before we rush to another solution, it is perhaps best to first investigate whether such approach is necessary.

Additionally , it's a good idea to check the average distance between the inscriptions and nearest cities (points) for all the cities within the convex hull to see how far from urban centers the inscriptions are on average. Would a small change to the buffer distance dramatically change the urban:rural ratio ?

*Instructions for duplicates*

* Use the `st_intersects()` function on the POLYGON feature of 399 individual buffers and the inscriptions to get a list of inscriptions per each of the 399 buffers. * Check the class of the list object. It should be a `sgbp` object, which is a “Sparse Geometry Binary Predicate”. It is a so-called sparse matrix, which is a list with integer vectors only holding the indices for each point that intersects the individual buffer polygon. 
* Calculate how many duplicates there are. (hint: `unique()` and `unlist()` functions can help you here). Just as a thought exercise, how would you get around the duplicates?

```{r}
## YOUR CODE
```


```{r overcounting, echo=FALSE, eval = FALSE}
clist <- st_intersects(cities_it, insc_sf3035)
class(clist)
clist

length(unique(unlist(clist)))  # 10462 are unique
length(which(duplicated(unlist(clist))))  # 1394

# Avoiding overcounting: 
# Select urban inscriptions by Voronyi polygons

nrow(insc_urban)
```
*Instructions for mean and median distance*

* Ensure the cities object has the same CRS as inscriptions object.
* Clip or select only those cities that fall within the convex hull of inscriptions to reduce the number of calculations. 
* Calculate the mean distance between the inscriptions and cities with `st_distance()`. Reduce the cities object only to those within the convex hull of inscriptions so as to reduce the calculation. Check this documentation to understand the output of the `st_nearest_feature()` function 
https://r-spatial.github.io/sf/reference/st_nearest_feature.html as to reduce computational intensity, it's best to check the distance between the nearest features only, rather than between all inscriptions and all cities.


```{r}
## YOUR CODE
```


```{r avg-distance, echo = FALSE}
# Distance to nearest feature is a 2 step process of first selecting the nearest town and then calculating the distance.
city_it_centroid <- st_centroid(cities_it)
dist_mat1 <- st_distance(insc_sf3035, cities_sf3035[st_nearest_feature(insc_sf3035, cities_sf3035$geometry),], by_element = TRUE)
dist_mat2 <- st_distance(insc_sf3035, city_it_centroid[st_nearest_feature(insc_sf3035, city_it_centroid),], by_element = TRUE)
# summary(dist_mat1)
# summary(dist_mat2)
# 
# hist(dist_mat1)
# hist(dist_mat2)
median(dist_mat2)

```

### Questions: 
3. How big is the overcounting problem? 
4. What is the average distance of all inscriptions from all the cities within the convex hull?
5. What can you say about the spatial distribution of ancient inscriptions vis-a-vis the cities? What factors might be impacting the distribution?


# Task 8: CHALLENGE - Map all the data with Leaflet
Let's now look at our inscriptions and subset of cities on some decent background, and remind ourselves how how to load polygons into Leaflet. What kind of CRS does Leaflet use again?

* Remember to use a consistent leaflet-compatible CRS
* Use `StamenWatercolor` provider tiles to create a simple, pretty map
* Don't forget `clusterOptions` argument to get a handle on the 12000+ points

```{r leaflet, echo = FALSE, eval=FALSE}
library(leaflet)
leaflet() %>% 
  addProviderTiles("Stamen.Watercolor") %>% 
  addPolygons(data = st_transform(insc_ch, crs=4326)) %>% 
  addCircleMarkers(data=insc_sf4326, clusterOptions = markerClusterOptions())
```
