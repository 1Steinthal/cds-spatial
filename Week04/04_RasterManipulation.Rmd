---
title: "04 - Raster manipulation"
author: "Adela Sobotkova"
date: "06/01/2021 updated`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r setup, include=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

# Task 1: Reduce the raster grid cell size using aggregate
Rasters, such as ortophotos, terrain models, or aggregated rasters for large area, often come is resolutions far greater than you need. Up til now you have played with small processed rasters. Now you are getting a taste of the real thing.  To reduce the computational load when running analyses, or when developing the right approach, you might wish to use a reduced resolution raster.

The function to reduce resolution in rasters is `aggregate()` which, as you might guess, aggregates grid cells into larger grid cells using a user-defined function (for example, mea
name	description
filetypen or max). The function used to aggregate the values is determined by the fun argument (the default being mean) and the amount of aggregation is driven by the fact (the default being 2) argument.

## Instructions

* Ensure that `raster` library is loaded
* Read in the elevation layer (it is a single-band raster and the file name is "Aster.tif").
* Plot the file you read in with `plot()`.
* Determine the raster resolution using `res()` and number of raster cells in the layer with `ncell()`.
* Aggregate the elevation raster using the default for fun and a factor of 10 and save the new raster to `elevation_small`.
* Plot the new raster with `plot()` for comparison to the old version.
* Determine the new raster resolution and the number of raster cells.

```{r aggr, eval=FALSE}
# Library
___(raster)

# Read in the elevation layer
elevation <- ___("data/Aster.tif")

# Plot the elevation raster
___(elevation)

# Determine the raster resolution
___(elevation)

# Determine the number of cells
___(elevation)

# Aggregate the raster
elevation_small <- ___(elevation, fact = ___)

# Plot the new elevation layer
___(elevation_small)

# Determine the new raster resolution
___(elevation_small)

# Determine the number of cells in the new raster
___(elevation_small)
```

### Solution
```{r aggr-sol, echo=FALSE}
# Library
library(raster)

# Read in the elevation layer
elevation <- raster("data/Aster.tif")

# Plot the elevation raster
plot(elevation)

# Determine the raster resolution
res(elevation)

# Determine the number of cells
ncell(elevation)

# Aggregate the raster
elevation_small <- aggregate(elevation, fact = 10)

# Plot the new elevation layer
plot(elevation_small)

# Determine the new raster resolution
res(elevation_small)

# Determine the number of cells in the new raster
ncell(elevation_small)
```

Excellent job! In this example you read in a raster and then converted it to a lower resolution raster to save on the size of the object and ultimately computation power required. In this example, the raster was not too big to begin with so perhaps aggregating would not be necessary but for big rasters this can be a big help.



# Task 2: Access raster data values
Raster data can be very big depending on the extent and resolution (grid size). In order to deal with this the `raster()` and `brick()` functions are designed to only read in the actual raster values as needed. To show that this is true, you can use the `inMemory()` function on an object and it will return `FALSE` if the values are not in memory. If you use the `head()` function, the `raster` package will read in only the values needed, not the full set of values. The raster values will be read in by default if you perform spatial analysis operations that require it or you can read in the values from a raster manually with the function `getValues()`.

## Instructions

* Use the `inMemory()` function on the elevation object to determine if the data has been read in.
* Use the `head()` function to look at the first few values from the elevation raster.
* Use the `getValues()` function on the elevation object to read in all the data.
* Use the `hist()` function to create a quick histogram of the elevation values. Note the pile of values near -9999, these should be `NA`  (any idea why?) and we will address this later.

```{r access raster, echo=TRUE, eval=FALSE}
# Check if the data is in memory
___(___)

# Use head() to peak at the first few records
___(___)

# Use getValues() to read the values into a vector
vals <- ___(elevation)

# Use hist() to create a histogram of the values
___(vals)

```

### Solution
```{r}
# Check if the data is in memory
inMemory(elevation)

# Use head() to peak at the first few records
head(elevation)

# Use getValues() to read the values into a vector
vals <- getValues(elevation)

# Use hist() to create a histogram of the values
hist(vals)
```


Congratulations! You now know that the `raster` package only reads in raster values as needed to save space in memory. You can get the values manually using the `getValues()` function.

Now, a new question arises, why are so many values encoded as -9999? Are we in the Marianna Trench all of sudden?


# Task 3: Change values and handle missing values in rasters
There are many situations where you might need to recode raster values. You may want to change outlier values to `NA` for example. In the `raster` package, reclassification is performed with the `reclassify()` function.

In the `elevation` raster you've worked with the values are percentages and are supposed to range between 0 and 2500. Anything below 0 should be an `NA`. In this exercise you will assign any values below 0 to `NA`.

## Instructions

* Check that the package `raster` and the object `elevation` are loaded in your workspace.
* Plot the `elevation` raster using `plot()`.
* Set up a three-column matrix with the `cbind()` function and values -10000, 0, NA.
* Use the matrix and `reclassify()` to reclassify values below 0 to `NA`. You will need to use the argument `rcl`.
* Plot the reclassified elevation layer to confirm there are no values below 0.

```{r reclass, eval=FALSE}
# Plot the elevation layer to see the values above 100
___(elevation)

# Set up the matrix
vals <- ___

# Reclassify 
elevation_reclass <- ___(elevation, ___ = ___)

# Plot again and confirm that the legend stops at 100
___(elevation_reclass)

```

### Solution
```{r reclass-sol, echo=FALSE}
# Plot the elevation layer to see the values above 100
plot(elevation)

# Set up the matrix
vals <- cbind(100, 300, NA)

# Reclassify 
elevation_reclass <-reclassify(elevation, rcl = vals)

# Plot again and confirm that the legend stops at 100
plot(elevation_reclass)
```

Good work! Knowing how to reclassify rasters will come in handy. When you get a chance you should review the help for `reclassify()` particularly the part that discusses how to specify the `rcl` argument. The three-column approach from this exercise is most common but there are other approaches.

# Task 4: Limit rasters to focus areas
Mask and crop are similar operations that allow you to limit your raster to a specific area of interest. With `mask()` you essentially place your area of interest on top of the raster and any raster cells outside of the boundary are assigned `NA` values. A reminder that in the past the `raster` package did not support `sf` objects so if you encounter difficulty with raster:vector interactions, it helps to convert the vector to `Spatial` object with, for example, `as(input, "Spatial")`.

## Instructions I

* Create `survey` object from shapefile ("survey.shp")
* Project the `survey` object to match the elevation raster with `st_transform()` and `crs()`. Assign this to `survey_cp`.
* Compute the area of the survey with `st_area()` and save this object as `areas`.
* Filter the survey units to only those above 30000 square meters with the `filter()` function. You will need to wrap `areas` in `unclass()`. Save as `survey_big`. Remember to have the tidyverse or dplyr library attached for `filter()` to work properly.

```{r filter-area, eval=FALSE, echo=TRUE}
# Read in the survey object
survey <- ___(___)

# Compute the area of the survey
areas <- ___(survey_cp)

# Filter to survey with areas > 30000
survey_big <- ___(survey_cp, ___ > 30000)

# Plot the elevation raster
plot(elevation)

# Plot the geometry of survey_big
___(___(survey_big))

# Convert survey to a Spatial object
survey_sp <- ___(survey_big, "Spatial")

# Mask the elevation layer with survey_sp and save as elevation_mask
elevation_mask <- ___(___, mask = survey_sp)
```

### Solution I
```{r filter-area-sol, echo=FALSE}
# Read in the survey object
survey <- st_read("data/survey.shp")

# Project survey to match elevation
survey_cp <- st_transform(survey, crs = crs(elevation, asText = TRUE))

# Compute the area of the survey
areas <- st_area(survey_cp)
class(unclass(areas))
# Filter to survey with areas > 30000
survey_big <- filter(survey_cp, unclass(areas) > 30000.0)

```
## Instructions II

* Review the plot of `elevation` raster.
* Plot the geometry of the `survey_big`.
* Convert the `survey_big` object to the `Spatial` class (the class from the package `sp`) with `as(input, "Spatial")` and save this as `survey_sp`.
* Mask the `elevation` layer with `survey_sp` and save as `elevation_mask`. This may take a couple of seconds.
* Review the plot of `elevation_mask`. It helps to additional `crop` the result to the extent of `survey_big` to zoom in on the survey units.
```{r plot-elevation, eval=FALSE}
# Plot the elevation raster
plot(________)

# Plot the geometry of survey_big
plot(_________(survey_big))

# Convert survey to a Spatial object
survey_sp <- as(survey_big, _____)

# Mask the elevation layer with survey_sp and save as elevation_mask
elevation_mask <- ________(elevation, mask = survey_sp)

# Plot elevation_mask -- this is a raster!
plot(elevation_mask)
```

### Solution II
```{r plot-elevation-sol,echo=FALSE }
# Plot the elevation raster
plot(elevation)

# Plot the geometry of survey_big
plot(st_geometry(survey_big))

# Convert survey to a Spatial object (no longer necessary in 2021, but good to remember for edge cases)
survey_sp <- as(survey_big, "Spatial")

# Mask the elevation layer with survey_sp and save as elevation_mask
elevation_mask <- mask(elevation, mask = survey_sp)  # with Spatial object
elevation_mask2 <- mask(elevation, mask = survey_big) # with sf object

# Plot elevation_mask -- this is a raster!
plot(elevation_mask) 
```

Nice! You transformed coordinates so that layers had the same CRS, you computed areas and filtered. You converted to a Spatial object and, finally, you used `mask()` to mask the elevation raster to the large survey units.



# Task 5: Crop a raster based on another spatial object
As you saw in the previous exercise with `mask()`, the raster extent is not changed. If the extents of the input raster and the mask itself are different then they will still be different after running `mask()`. In many cases, however, you will want your raster to share an extent with another layer and this is where `crop()` comes in handy. With `crop()` you are cropping the raster so that the extent (the bounding box) of the raster matches the extent of the input crop layer. But within the bounding box no masking is done (no raster cells are set to `NA`).

In this exercise you will both mask and crop the Kazanlak elevation layer based on the large survey units and you'll compare the results. You should notice that the masked raster includes a lot of `NA` values (they are the whitespace) and that the extent is the same as the original elevation layer. With the cropped layer you should notice that the extent of the cropped elevation layer matches the extent of the large survey (essentially it's zoomed in).

## Instructions

* Plot the `elevation_mask` object.
* Crop the `elevation` layer using the `survey_sp` layer with `crop()`.
* Plot the cropped layer and see how the edges go right to the axis lines. Toggle between the plots to compare them.

```{r raster-crop, eval=FALSE, echo=TRUE}
# Mask the elevation with the large survey 
elevation_mask <- ___(elevation, mask = survey_sp)

# Plot the mask
___

# Crop elevation with survey_sp
elevation_crop <- ___(___, survey_sp)

# Plot the cropped version and compare
___(elevation_crop)
```

### Solution
```{r raster-crop-sol, echo=FALSE}
# Plot the mask
plot(elevation_mask)

# Crop elevation with survey_sp
elevation_crop <- crop(elevation, survey_sp)

# Plot the cropped version and compare
plot(elevation_crop)

```

Now you should know the difference between mask and crop. With mask the extents remain unchanged but all raster values outside the mask are set to `NA`. With crop the raster extent is cropped to match the other layer.


# Task 6: Extract raster values by location
Beyond simply masking and cropping you may want to know the actual cell values at locations of interest. You might, for example, want to know the percentage elevation at your sites or within the large survey. This is where the `extract()` function comes in handy.

Usefully, and you'll see this in a later analysis, you can feed `extract()` a function that will get applied to extracted cells. For example, you can use `extract()` to extract raster values by neighborhood and with the `fun = mean` argument it will return an average cell value by neighborhood.

Similar to other `raster` functions, it is not yet set up to accept `sf` objects so you'll need to convert to a `Spatial` object.

## Instructions

* Create `sites` object out of the "sitecentroids4326.csv" dataframe with `st_as_sf()`.
* Project the `sites` points to match the `elevation` CRS. 
* Use the `raster` function `extract()` to determine the mean elevation at each of the sites. Save this as `sites_ex`.
* Look at the `sites_cp` and `sites_ex` objects in the console. Do the `extract()` results make sense? The `elevation` layer values represent meters above sea level.

```{r extract, eval=FALSE}
# Read in sites dataframe
sites_df <- read_csv(_____)

# Look for coordinate columns
sides_df

# Convert the data frame to an sf object             
sites <- _____(sites_df, coords = c(,), crs =4326)

# Project the sites to match elevation
sites_cp <- ___(sites, ___ = ___(elevation, ___ = ___))

# Extract the elevation values at the sites
sites_cp$elev  <- _______::______(elevation, sites_cp)

# Look at the sites and extraction results

```

### Solution
```{r extract-sol, echo=FALSE,eval=FALSE}

# # Instructor note: prepare a csv out of the sf object
# st_write(st_transform(st_centroid(sites), crs = 4326), "data/sitecentroids4326.csv", layer_options = "GEOMETRY=AS_XY")

# Create a sites dataframe
sites_df <- read_csv("data/sitecentroids4326.csv")
  
# Review sites df
sites_df

# Convert the data frame to an sf object             
sitecentroids <- st_as_sf(sites_df, coords = c("X", "Y"), crs =4326)

# Project the sites to match elevation
sites_cp <- st_transform(sitecentroids, crs = st_crs(elevation, asText = TRUE))

# Extract the elevation values at the sites
sites_cp$elev <- raster::extract(elevation, sites_cp)

# Look at the sites and extraction results
plot(sites_cp["elev"])
sites_cp$elev
```

Great! `extract()` is a very useful tool. It can be used with polygons as well as points, and the result can be written out as a separate vector or added to an existing object as a column. 


# Task 7: Raster math with overlay
You will now use the `elevation` layer and an "prominence" layer. Prominence measures what percentage of surrounding cells are visible from any given location in the raster. So a high percentage indicates a prominent point, while a low percentage indicates a low-lying location with poor inter-visibility.

What you will do in this exercise is essentially identify the most prominent locations among the registered archaeological sites by finding areas that have both a high percentage of prominence and high elevation. To do this, you will define an overlay function `f` to do the raster math with.

## Instructions

* Make sure elevation object exists in memory ("Aster.tif", it is a single-band raster).
* Read in the prominence layer ("prominence.tif", it is also a single-band raster).
* Specify function f, where you look for elevation values over 400msl and prominence values over 50%
* Call `overlay()` on `elevation` and `prominence`. Set the `fun` argument to `f`.


```{r overlay, eval=FALSE, echo=TRUE}
# Check in on the elevation and read in prominence layer
elevation
prominence <- ___(___)

# Function f with 2 arguments and the raster math code
f <- function(rast1, rast2) {
    ___________
}

# Do the overlay using f as fun
elevation_prom_overlay <- ___(elevation, ___, fun = ___)


# Plot the result (low elevation and high prominence areas)
___(elevation_prom_overlay)
```

### Solution
```{r overlay-sol, echo=FALSE}
# Read in the elevation and prominence layer
elevation
prominence <- raster("data/prominence1500m.tif")

# Function f with 2 arguments and the raster math code
f <- function(rast1, rast2) {
  rast1 > 400 & rast2 > 50
}

# Do the overlay using f as fun
elevation_prom_overlay <- overlay(crop(elevation, prominence),prominence, fun = f)

# Plot the result (high elevation and high prominence areas)
plot(elevation_prom_overlay)
```

Congratulations! You've now learned to perform raster math using the raster function `overlay()`. You limited to areas with > 400m  elevation and > 60% prominence, these areas will be the most defense-ready areas in Kazanlak.

# Task 8: Zoom on the overlay results

In the above exercise, we produced an elevation-prominence overlay. Sites that sit in this overlay enjoy a strategic position vis-a-vis the rest of the valley. Which ones are they, however?  It is hard to see at the scale of the valley and it would be good to pull the sites out. 

You have learnt to apply `mask` and `crop` in sections 4 and 5 above. Apply these to limit the raster results to the polygon features.  

## Instructions

* Create a `sites` sf object by reading in polygons from "KAZ_nuclei.shp"
* Plot the elevation-prominence overlay and the sites on top of each other
* Use `mask` function to create a sites_mask raster, where raster cells under sites' polygons retain their original values, while cells outside the polygons are turned to `NA`.
* Use `crop` function to crop sites_masked raster to the extent of the sites polygons.
* Plot the result
* Optional: how would you produce a list of sites that have at least 0.5 ha (ie. 5 grid cells) within these strategic high-visibility locations? (hint:you could first extract a list of overlapping sites and then use `sapply()` to filter out the ones > 5)

```{r mask-crop,eval = FALSE}
# Read in the site polygons
sites <- ______("data/KAZ_nuclei.shp")

# Plot the sites
plot(elevation_prom_overlay);plot(__________)


# Create a raster mask from the elevation_prom_overlay using the sites
sites_mask <- _____________

# Crop the raster mask to the sites
sites_strat <- _____________

# Plot and compare the results
plot(sites_mask)
plot(sites_strat)


# Optional: produce a list of the most prominent sites

```

### Solution
```{r mask-crop, echo = FALSE}
# Read in the site polygons
sites <- st_read("data/KAZ_nuclei.shp")

plot(elevation_prom_overlay);plot(sites$geometry, add = TRUE, col = "red")

sites_mask <- mask(elevation_prom_overlay, sites)
hist(as.numeric(values(sites_masked)))

sites_strat <- crop(sites_mask, sites)
plot(sites_strat)

sites_elmask <- mask(elevation, sites)

# Which ones overlap?
defenselist <- raster::extract(sites_strat, sites)
sapply(defenselist, sum)
prom_sites <- sites[which(sapply(defenselist, sum)>5),]
prom_sites$TRAP_Code
```
Lovely work! Manipulating rasters and mastering raster-vector interactions is the bread and butter of spatial data wrangling :)
